{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip && pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "import boto.s3.connection\n",
    "from boto.s3.key import Key\n",
    "import os.path\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Initialize variables\n",
    "# #############################################################################\n",
    "input_file = \"topics.csv\"\n",
    "output_file = \"text-classifier.pkl\"\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# S3 CONFIGURATION\n",
    "# #############################################################################\n",
    "default_access_key = 'xxxxx' # putting access key\n",
    "default_secret_key = 'xxxxxx' # putting secret key\n",
    "default_bucket_name = 'xxxxxxxx' # bucket name\n",
    "default_host = 'xxxxxxx'\n",
    "\n",
    "def saveFileToDisk(s3_host, s3_access_key, s3_secret_key, bucket_name, key_name,file_name):\n",
    "    conn = boto.connect_s3 (\n",
    "        aws_access_key_id = s3_access_key,\n",
    "        aws_secret_access_key = s3_secret_key,\n",
    "        host = s3_host,\n",
    "        is_secure=False, \n",
    "        calling_format = boto.s3.connection.OrdinaryCallingFormat(),\n",
    "    )\n",
    "    bucket = conn.get_bucket(bucket_name)\n",
    "    key = bucket.get_key(key_name)\n",
    "    key.get_contents_to_filename(file_name)\n",
    "    print(file_name + ' downloaded.')\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Load some categories from the training set\n",
    "if not path.exists(input_file):\n",
    "    saveFileToDisk(default_host,default_access_key,default_secret_key, default_bucket_name, input_file, input_file)\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "original_headers = list(df.columns.values)\n",
    "\n",
    "categories = df[original_headers[2]].unique().tolist()\n",
    "\n",
    "print(\"Total dataset size: \"+str(len(df)))\n",
    "\n",
    "# Split data: 80% for training, 20% for testing\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "print(\"Training dataset size: \"+str(len(train)))\n",
    "print(\"Test dataset size: \"+str(len(test)))\n",
    "\n",
    "train_x =  train[original_headers[1]].tolist()\n",
    "train_y = train[original_headers[2]].tolist()\n",
    "\n",
    "test_x = test[original_headers[1]].tolist()\n",
    "test_y = test[original_headers[2]].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Extracting features from text files\n",
    "# #############################################################################\n",
    "\n",
    "# Tokenizing text\n",
    "# https://stackabuse.com/text-classification-with-python-and-scikit-learn/\n",
    "count_vect = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X_train_counts = count_vect.fit_transform(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Apply TFIF\n",
    "# #############################################################################\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Create a simple naive bayes\n",
    "# #############################################################################\n",
    "clf = MultinomialNB().fit(X_train_tfidf, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Test simple naive bayes\n",
    "# #############################################################################\n",
    "X_test_counts = count_vect.transform(test_x)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "    \n",
    "clf_predicted = clf.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Accuracy: {0:.2f} %\".format(100 * accuracy_score(test_y, clf_predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############################################################################\n",
    "# Save model to a pickle file\n",
    "# #############################################################################\n",
    "with open(output_file, 'wb') as file:\n",
    "    pickle.dump(clf, file)\n",
    "print(\"Model file: \"+output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
